{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.!!!!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "\n",
    "print('Libraries imported.!!!!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing all standard libraries and modules , we import BEatiful Soup for scraping Wikipedia page for Toronto. (as from Turkey we can not reach Wikipedia (dont ask me why) we reach WikiZero and use that url ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beatiful Soup Imported\n",
      "Requirement already satisfied: lxml in /home/jupyterlab/conda/lib/python3.6/site-packages (4.3.0)\n",
      "lxml installed \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "print(\"Beatiful Soup Imported\")\n",
    "!pip install lxml\n",
    "print(\"lxml installed \")\n",
    "!wget -q -O 'html_doc_toronto.html' http://www.wikizero.biz/index.php?q=aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTGlzdF9vZl9wb3N0YWxfY29kZXNfb2ZfQ2FuYWRhOl9N\n",
    "with open('html_doc_toronto.html') as fp:\n",
    "    soup = BeautifulSoup(fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find 'table' by using soup.find \n",
    "Th_tags id from finding Table headings  which create Column Names by using get_text function\n",
    "We define a DataFrame named dfToronto to put all data into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toronto DataFrame with Not Assigned Data   shape :  (288, 3)\n"
     ]
    }
   ],
   "source": [
    "table = soup.find( \"table\", {\"class\":\"wikitable sortable\"}) \n",
    "rows=list()\n",
    "column_names = [] \n",
    "n_columns = 0\n",
    "n_rows=0\n",
    "th_tags = table.find_all('th')\n",
    "tr_tags = table.find_all('tr')\n",
    "for th in th_tags:\n",
    "    column_names.append(th.get_text())\n",
    "    \n",
    "columns = column_names if len(column_names) > 0 else range(0,n_columns)\n",
    "for row in table.find_all('tr'):\n",
    "    \n",
    "     # Determine the number of rows in the table\n",
    "    td_tags = row.find_all('td')\n",
    "    \n",
    "    if len(td_tags) > 0:\n",
    "        n_rows+=1\n",
    "        if n_columns == 0:\n",
    "                        # Set the number of columns for our table\n",
    "            n_columns = len(td_tags)\n",
    "    \n",
    "\n",
    "dfToronto = pd.DataFrame(columns = columns,\n",
    "                        index= range(0,n_rows))\n",
    "print(\"Toronto DataFrame with Not Assigned Data   shape : \",dfToronto.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each tr (table record) in the Table \n",
    "    Find Td (table data) and put every TD to Toronto DataFrame Columns(use .iat)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toronto w/o  Aggregation shape :  (211, 3)\n"
     ]
    }
   ],
   "source": [
    "row_marker=0\n",
    "column_marker=0\n",
    "for row in table.find_all('tr'):\n",
    "    column_marker = 0\n",
    "    columns = row.find_all('td')   \n",
    "    for column in columns:\n",
    "        dfToronto.iat[row_marker,column_marker] = column.get_text()\n",
    "        column_marker += 1\n",
    "        \n",
    "    if len(columns) > 0:\n",
    "        row_marker += 1\n",
    "#for col in dfToronto:\n",
    " #   try:\n",
    " #       dfToronto[col] = dfToronto[col].astype(float)\n",
    " #   except ValueError:\n",
    " #       pass\n",
    "print(\"Toronto w/o  Aggregation shape : \",dfToronto_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cretae a new DataFrame dfToronto_new  with \n",
    "    If Neighborhood is 'not Assigned' make Neighborhhod same as Borough\n",
    "    If Borough is Not Assigned skip that row !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "dfToronto_new=dfToronto[dfToronto.Borough != 'Not assigned']\n",
    "dfToronto_new['Neighbourhood\\n'] = np.where(dfToronto_new['Neighbourhood\\n'] == 'Not assigned\\n', \n",
    "                                            dfToronto_new['Borough'], dfToronto_new['Neighbourhood\\n'])\n",
    "print(dfToronto_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Aggregate Neighborhoods Let's group by DataFrame PostCode and Borough and use Aggregate function to concanate the Neighborhood data by using lambda function\n",
    "dfToronto_newer is the final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toronto w/o  Aggregation shape :  (211, 3)\n",
      "Toronto with Aggregation shape :  (103, 3)\n"
     ]
    }
   ],
   "source": [
    "unionofneigh = lambda a: \",\".join(a)\n",
    "\n",
    "dfToronto_newer=dfToronto_new.groupby(['Postcode','Borough'],as_index=False).agg({'Neighbourhood\\n': unionofneigh})\n",
    "print(\"Toronto w/o  Aggregation shape : \",dfToronto_new.shape)\n",
    "print(\"Toronto with Aggregation shape : \",dfToronto_newer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
